## Лаба 3. Классифицировать интернет-пользователей на группы по логу посещения сайтов с помощью Apache Hive

##### [![New Professions Lab — Big Data 10](http://data.newprolab.com/public-newprolab-com/npl7.svg)](https://github.com/newprolab/content_bigdata10)

### Дедлайн

⏰ Четверг, 18 апреля 2019 года, 23:59.

### Задача

Серьезный прирост ценности DMP-платформы заключается и в том, чтобы из имеющихся сырых данных провести профилирование пользователей. У некоторых DMP-систем имеется порядка 1 500 различных профилей пользователей.

После этого приходит клиент, продающий какой-то конкретный продукт и знающий свою целевую аудиторию. DMP-система будет показывать баннеры только нужным посетителям.

В этой лабораторной работе **вам предстоит использовать Apache Hive** (с помошью утилиты `beeline`) для категоризации пользователей. Как вы знаете, Hive преобразует SQL-подобный запрос в серию MapReduce джобов — проверить, что вы использовали именно Hive мы, очевидно, не станем, но крайне рекомендуем воспользоваться именно этим инструментом.

#### Структура данных Лабы 3 на HDFS кластера

Исходные файлы расположены в директории HDFS по адресу `/labs/lab03data`.

Каждый файл представляет собой коллекцию записей, по записи на строчку, где каждая строка представляет собой кортеж `(UID, timestamp, URL)`:

* `UID` — уникальный идентификатор пользователя, представленый натуральным числом, записанным в десятичной форме. 

* `timestamp` — отметка времени, записанная в форме UNIX timestamp, записана в виде десятичной дроби.

* `URL` — экранированный URL, представлен в виде строки. Записи разделены символом табуляции `\t`.

Пример:
`26153949061	1422751272.768	http%3A%2F%2Frzd.ru%2F`

#### Обработка данных на вход

Для выполнения работы **вам следует взять все файлы** из HDFS: `/labs/lab03data`.

Их около полугигабайта:

```
hadoop fs -du -h -s /labs/lab03data
657.8 M  /labs/lab03data
```

В личном кабинете у вас будут указаны наименования 4 категорий пользователей и перечень доменов к ним (3 домена).

Пользователь принадлежит к какой-либо категории, если он посетил **хотя бы один из доменов категории хотя бы 10 раз.**

Пример. Существует категория пользователей “Искатель”. Перечень доменов для нее следующий:

* `ya.ru`;

* `google.ru`;

* `market.ya.ru`.

Домен 3-его уровня (`market.ya.ru` и другие аналогичные) считать отдельным самостоятельным доменом. В то же время домен с www и без www — это один и тот же домен.

Если пользователь посетил по 4 раза каждый домен из примера, то он не будет относиться к указанной категории. А если он посетил только один домен 10 раз — то будет.

Брать в рассмотрение только `url` с `http://` и `https://`. Классифицировать только тех пользователей, у которых `id` является непустым.

#### Обработка данных на выход

⚠️ Важно: Выходной файл должен быть расположен **НЕ в HDFS**, а в вашей домашней директории на master.cluster-lab.com в файле `lab03_users.txt`. Чекер будет брать файл именно оттуда.

Содержимое файла должно представлять собой список строк для всех пользователей, встречающихся в исходных данных, без заголовка столбцов.

Поля: `uid, user_cat1_flag, user_cat2_flag, user_cat3_flag, user_cat4_flag`, разделённые табуляцией. Значения полей `user_cat1_flag-user_cat4_flag` принимают значения `0` и `1` (`0` — пользователь не относится к соответствующей категории, `1` — пользователь относится к соответствующей категории). Пользователь может относиться одновременно к нескольким категориям или не относиться ни к одной.

Порядок полей `user_cat1_flag-user_cat4_flag` должен соответствовать порядку категорий в личном кабинете.

Пользователи должны быть отсортированы по `UID` по возрастанию значений (трактовать как число).

Пример файла.
```
123	0	0	0	0	
456	1	1	1	1	
789	0	0	0	0		
```

#### Подсказки
0. Как запускать beeline и подключаться к базе смотрите [в файле-подсказке](/important_sites.md)

1. Для выделения домена из `url` можно воспользоваться следующей функцией. Код страшноватый, но рабочий:

```python
from urllib.parse import urlparse, unquote
import re
def url2domain(url):
   try:
       a = urlparse(unquote(url.strip()))
       if (a.scheme in ['http','https']):
           b = re.search("(?:www\.)?(.*)",a.netloc).group(1)
           if b is not None:
               return str(b).strip()
           else:
               return ''
       else:
           return ''
   except:
       return
```

Файл рекомендуется сохранять в UTF-8, конец строки в Unix/Linux формате.

2. На семинаре мы создавали отдельную базу данных для каждого пользователя:
```
create database apilipenko3;
use database apilipenko3
```

Но из-за непонятного бага(?) админу приходилось корректировать права доступа после создания базы. Предлагаю создавать таблицу в default базе, то есть начинать работу с `CREATE TABLE name_surname ...`

Назовите таблицу в Hive в соответствие с вашим логином в ЛК, только вместо точки - подчерк, например: ivan_ivanov

3. Сохранение данных запроса возможно двумя способами:

3a.
```
INSERT OVERWRITE DIRECTORY 'hdfs://master.cluster-lab.com:8020/user/name.surname/lab03result'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
SELECT ...
FROM ...
GROUP BY uid;
```

ваши файлы сохранятся на HDFS в `/user/name.surname/lab03result`. забрать из оттуда можно штатными средствами HDFS: `hdfs dfs -cat /user/name.surname/lab03result/* > ~/lab03_users.txt`. При повторном запуске запроса не забудьте удалить /user/name.surname/lab03result

3b. Можно поместить запрос (только часть с SELECT) в файл и запустить beeline в пакетном режиме:

```
beeline -u jdbc:hive2://node1.cluster-lab.com:10000 -n name.surname --outputformat=csv2 --silent=true -f solution1.hql > ~/lab03_users_commas.txt
```
Правда, в таком случае, в файле вместо табов будут запятые. Можно их заменить с помощью команд юникса:

```
cat ~/lab03_users_commas.txt | tr ',' '\t' > ~/lab03_users.txt
```

4. Не забудьте сохранить код SQL который вы используете в Hive! Пишите весть код для Hive в отдельном фале и делайте копи-паст оттуда в командную строку beeline.

#### Проверка

Проверка осуществляется автоматическим скриптом из личного кабинета.


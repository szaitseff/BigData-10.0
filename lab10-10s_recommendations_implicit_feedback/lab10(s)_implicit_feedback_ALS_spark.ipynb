{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лаба 10. Построить рекомендательную систему видеоконтента с implicit feedback\n",
    "\n",
    "<img width=\"350px\" src=\"images/megafon_logo.jpg\">\n",
    "\n",
    "##### Лаба создана при поддержке компании «МегаФон». \n",
    "\n",
    "### Дедлайн\n",
    "\n",
    "⏰ Четверг, 20 июня 2019 года, 23:59.\n",
    "\n",
    "### Задача\n",
    "\n",
    "В вашем распоряжении имеется уже предобработанный и очищенный датасет с фактами\n",
    "покупок абонентами телепередач от компании E-Contenta. По доступным вам данным нужно предсказать вероятность покупки других передач этими, а, возможно, и другими абонентами.\n",
    "\n",
    "### Обработка данных на вход\n",
    "\n",
    "Для выполнения работы вам следует взять все файлы из папки на HDFS `/labs/lab10data/`. Давайте посмотрим, что у нас есть:\n",
    "\n",
    "```\n",
    "$ hadoop fs -ls /labs/lab10data\n",
    "Found 4 items\n",
    "-rw-r--r--   3 hdfs hdfs   91066524 2017-05-09 13:51 /labs/lab10data/lab10_items.csv\n",
    "-rw-r--r--   3 hdfs hdfs   29965581 2017-05-09 13:50 /labs/lab10data/lab10_test.csv\n",
    "-rw-r--r--   3 hdfs hdfs   74949368 2017-05-09 13:50 /labs/lab10data/lab10_train.csv\n",
    "-rw-r--r--   3 hdfs hdfs  871302535 2017-05-09 13:51 /labs/lab10data/lab10_views_programmes.csv\n",
    "```\n",
    "\n",
    "* В `lab10_train.csv` содержатся факты покупки (колонка `purchase`) пользователями (колонка `user_id`) телепередач (колонка `item_id`). Такой формат файла вам уже знаком.\n",
    "\n",
    "* `lab10_items.csv` — дополнительные данные по items. В данном файле много лишней или ненужной информации, так что задача её фильтрации и отбора ложится на вас. Поля в файле, на которых хотелось бы остановиться:\n",
    "  * `item_id` — primary key. Соответствует `item_id` в предыдущем файле.\n",
    "  * `content_type` — тип телепередачи (`1` — платная, `0` — бесплатная). Вас интересуют платные передачи.\n",
    "  * `title` — название передачи, текстовое поле.\n",
    "  * `year` — год выпуска передачи, число.\n",
    "  * `genres` — поле с жанрами передачи, разделёнными через запятую.\n",
    "* `lab10_test.csv` — тестовый датасет без указанного целевого признака `purchase`, который вам и предстоит предсказать.\n",
    "* Дополнительный файл `lab10_views_programmes.csv` по просмотрам передач с полями:\n",
    "  * `ts_start` — время начала просмотра\n",
    "  * `ts_end` — время окончания просмотра\n",
    "  * `item_type`— тип просматриваемого контента:\n",
    "    * `live` — просмотр \"вживую\", в момент показа контента в эфире\n",
    "    * `pvr` — просмотр в записи, после показа контента в эфире\n",
    "\n",
    "\n",
    "### Обработка данных на выход\n",
    "\n",
    "Предсказание целевой переменной \"купит/не купит\" — хорошо знакомая вам задача бинарной классификации, с которой вы уже встречались в [Лабе 4](../../labs/lab04/lab04.md). Поскольку нам важны именно вероятности отнесения пары `(пользователь, товар)` к классу \"купит\" (`1`), то, на самом деле, вы можете подойти к проблеме с разных сторон:\n",
    "1. Как к разработке рекомендательной системы: рекомендовать пользователю `user_id` топ-N лучших телепередач, которые были найдены по методике user-user / item-item коллаборативной фильтрации.\n",
    "2. Как к задаче факторизации матриц: алгоритмы SVD, ALS, FM/FFM.\n",
    "3. Как просто к задаче бинарной классификации. У вас есть два датасета, которые можно каким-то образом объединить, дополнительно обработать и сделать предсказания классификаторами (Apache Spark, pandas + sklearn на ваше усмотрение).\n",
    "4. Как к задаче регрессии. Поскольку от вас требуется предсказать не факт покупки, а его *вероятность*, то можно перевести задачу в регрессионную и решать её соответствующим образом. \n",
    "\n",
    "### Подсказки\n",
    "\n",
    "1. Кроссвалидация — ваш друг. Используйте `pyspark.ml.tuning.TrainValidationSplit` вместе с `ParamGridBuilder`, чтобы произвести grid search гиперпараметров вашей модели.\n",
    "2. Простой подсчёт ROC AUC в Apache Spark доступен в `pyspark.ml.evaluation.BinaryClassificationEvaluator`.\n",
    "\n",
    "### Проверка\n",
    "\n",
    "Мы будем оценивать ваш алгоритм по метрике ROC AUC. Ещё раз напомним, что чекеру требуются *вероятности* в диапазоне `[0.0, 1.0]` отнесения пары `(пользователь, товар)` в тестовой выборке к классу \"1\" (купит).\n",
    "\n",
    "Для успешного прохождения лабораторной работы **AUC должен составить не менее 0.6**.\n",
    "\n",
    "**Важно!** Для точной проверки не забудьте отсортировать полученный файл по возрастанию идентификаторов пользователей (`user_id`), а затем — по возрастанию идентификаторов передач (`item_id`). Образец - `lab10_test.csv`\n",
    "\n",
    "Результат следует сохранить в файл `lab10.csv` в своей домашней директории.\n",
    "\n",
    "Проверка осуществляется [автоматическим скриптом](http://lk.newprolab.com/lab/laba10) из Личного кабинета.\n",
    "\n",
    "## Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Запуск pyspark\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir('checkpoint/')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Прокидываем порты\n",
    "!ssh bdmaster -L 8088:localhost:8088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2018-11-27 16:00 /labs/lab10data/lab10_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2018-11-27 16:00 /labs/lab10data/lab10_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2018-11-27 16:00 /labs/lab10data/lab10_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2018-11-27 16:00 /labs/lab10data/lab10_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /labs/lab10data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lab10_train.csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!hadoop fs -head /labs/lab10data/lab10_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|       0|\n",
      "|   1654|  89249|       0|\n",
      "|   1654|  99982|       0|\n",
      "|   1654|  89901|       0|\n",
      "|   1654| 100504|       0|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(fields=[StructField(\"user_id\", IntegerType()),\n",
    "                            StructField(\"item_id\", IntegerType()),\n",
    "                            StructField(\"purchase\", IntegerType())])\n",
    "\n",
    "train = spark.read.csv('/labs/lab10data/lab10_train.csv', schema=schema, header=True)\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+--------------------+\n",
      "|summary|           user_id|          item_id|            purchase|\n",
      "+-------+------------------+-----------------+--------------------+\n",
      "|  count|           5032624|          5032624|             5032624|\n",
      "|   mean| 869680.9464782189|66869.30485865823|0.002166662957534...|\n",
      "| stddev|60601.098215631355|35242.28205538276| 0.04649697795291635|\n",
      "|    min|              1654|              326|                   0|\n",
      "|    25%|            846231|            65667|                   0|\n",
      "|    50%|            885247|            79853|                   0|\n",
      "|    75%|            908588|            93606|                   0|\n",
      "|    max|            941450|           104165|                   1|\n",
      "+-------+------------------+-----------------+--------------------+\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%time train.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT user_id)|\n",
      "+-----------------------+\n",
      "|                   1941|\n",
      "+-----------------------+\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 5.49 s\n"
     ]
    }
   ],
   "source": [
    "# Количество уникальных юзеров\n",
    "train.agg(countDistinct(\"user_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT item_id)|\n",
      "+-----------------------+\n",
      "|                   3704|\n",
      "+-----------------------+\n",
      "\n",
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 2.6 s\n"
     ]
    }
   ],
   "source": [
    "# Количество уникальных шоу\n",
    "train.agg(countDistinct(\"item_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|purchase|  count|\n",
      "+--------+-------+\n",
      "|       1|  10904|\n",
      "|       0|5021720|\n",
      "+--------+-------+\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "# Количество покупок\n",
    "%time train.groupBy(\"purchase\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lab10_test.csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!hadoop fs -head /labs/lab10data/lab10_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  94814|    null|\n",
      "|   1654|  93629|    null|\n",
      "|   1654|   9980|    null|\n",
      "|   1654|  95099|    null|\n",
      "|   1654|  11265|    null|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(fields=[StructField(\"user_id\", IntegerType()),\n",
    "                            StructField(\"item_id\", IntegerType()),\n",
    "                            StructField(\"purchase\", IntegerType())])\n",
    "\n",
    "test = spark.read.csv('/labs/lab10data/lab10_test.csv', schema=schema, header=True)\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------+\n",
      "|summary|          user_id|           item_id|purchase|\n",
      "+-------+-----------------+------------------+--------+\n",
      "|  count|          2156840|           2156840|       0|\n",
      "|   mean|869652.3733920922| 66896.00283609354|    null|\n",
      "| stddev|60706.51616335023|35227.831307045984|    null|\n",
      "|    min|             1654|               326|    null|\n",
      "|    25%|           846164|             65668|    null|\n",
      "|    50%|           885124|             79856|    null|\n",
      "|    75%|           908588|             93606|    null|\n",
      "|    max|           941450|            104165|    null|\n",
      "+-------+-----------------+------------------+--------+\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%time test.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT user_id)|\n",
      "+-----------------------+\n",
      "|                   1941|\n",
      "+-----------------------+\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "# Те же самые юзеры\n",
    "test.agg(countDistinct(\"user_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT item_id)|\n",
      "+-----------------------+\n",
      "|                   3704|\n",
      "+-----------------------+\n",
      "\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "# Те же самые шоу\n",
    "test.agg(countDistinct(\"item_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"purchase\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Journal\n",
    "\n",
    "maxIter | regParam | rank | alpha | nonnegative | rocauc_train | rocauc_test | max_test_conf | time | timestamp\n",
    ":--- | :---: | :---: | :---: | :---: | :---: | :--- | :--- | :---\n",
    "10 | 0.1 | 10 | 1.0 |False | 0.9671 | 0.7778 | 0.97? | 28s | 2019-14-06 12:29\n",
    "10 | 1.0 | 10 | 1.0 | True | 0.9637  | 0.7986 | 0.27 | 28s | 2019-14-06 19:31 \n",
    "10 | 1.0 | 10 | 40.0 | True | 0.9870 | 0.7885 | 2.31  | 28s | 2019-14-06 20:34\n",
    "10 | 1.0 | 10 | 10.0 | True | 0.9795 | 0.8002  | 1.13 | 28s | 2019-14-06 21:22\n",
    "10 | 10.0 | 10 | 10.0 | True | 0.8888 | 0.7381  | 0.005 | 28s | 2019-14-06 22:03\n",
    "10 | 2.0 | 10 | 10.0 | True | 0.9788 | **0.8058**  | 0.82 | 28s | 2019-14-06 22:42\n",
    "10 | 3.0 | 10 | 10.0 | True | 0.9764 | 0.8046  | 0.63 | 16s | 2019-14-06 22:52\n",
    "10 | 2.0 | 20 | 10.0 | True | 0.9888 | 0.8041  | 0.83 | 13s | 2019-14-06 23:05\n",
    "10 | 3.0 | 20 | 10.0 | True | 0.9878 | 0.8053  | 0.65 | 8s | 2019-14-06 23:13\n",
    "10 | 3.0 | 30 | 10.0 | True | 0.9916 | 0.8026  | 0.66 | 7s | 2019-14-06 23:20\n",
    "10 | 4.0 | 30 | 10.0 | True | 0.9905 | 0.8029  | 0.51 | 7s | 2019-14-06 23:26\n",
    "10 | 4.0 | 40 | 10.0 | True | 0.9927 | 0.8035  | 0.48 | 7s | 2019-14-06 23:34\n",
    "10 | 4.0 | 50 | 20.0 | True | 0.9974 | 0.8011  | 0.81 | 8s | 2019-14-06 23:41\n",
    "10 | 5.0 | 50 | 20.0 | True | 0.9972 | 0.8012  | 0.72 | 8s | 2019-14-06 23:46\n",
    "10 | 6.0 | 50 | 20.0 | True | 0.9968 | 0.8009  | 0.65 | 8s | 2019-14-06 23:52\n",
    "20 | 6.0 | 50 | 20.0 | True | 0.9969 | 0.8008  | 0.66 | 10s | 2019-14-06 23:58\n",
    "20 | 6.0 | 60 | 20.0 | True | 0.9975 | 0.8013  | 0.66 | 13s | 2019-15-06 00:04\n",
    "20 | 6.0 | 70 | 20.0 | True | 0.9980 | 0.8012  | 0.68 | 13s | 2019-15-06 00:13\n",
    "20 | 2.0 | 10 | 10.0 | True | 0.9788 | 0.8084  | 0.87 | 35s | 2019-15-06 23:26\n",
    "20 | 2.0 | 10 | 10.0 | False | 0.9840 | 0.7952  | 0.83/clamp | 14s | 2019-15-06 23:35\n",
    "20 | 2.0 | 10 | 10.0 | False | 0.9840 | 0.8189  | 0.83 | 14s | 2019-15-06 23:38\n",
    "20 | 2.0 | 8 | 10.0 | False | 0.9801 | 0.8208  | 0.81 | 14s | 2019-15-06 23:48\n",
    "20 | 2.0 | 7 | 10.0 | False | 0.9776 | 0.8210  | 0.80 | 42s | 2019-16-06 19:25\n",
    "20 | 2.0 | **6** | 10.0 | False | 0.9739 | **0.8286**  | 0.83 | 23s | 2019-16-06 19:36\n",
    "20 | 2.0 | 5 | 10.0 | False | 0.9690 | 0.8198  | 0.80 | 18s | 2019-16-06 19:46\n",
    "20 | 2.0 | 6 | **5.0** | False | 0.9687 | **0.8330**  | 0.49 | 18s | 2019-16-06 19:58\n",
    "20 | 2.0 | 6 | 4.0 | False | 0.9672 | 0.8315  | 0.37 | 1m6s | 2019-16-06 21:57\n",
    "20 | 2.0 | 6 | 6.0 | False | 0.9699 | 0.8315  | 0.58 | 23s | 2019-16-06 22:08\n",
    "20 | 2.5 | 6 | 5.0 | False | 0.9680 | 0.8325  | 0.37 | 16s | 2019-16-06 22:56\n",
    "20 | 1.5 | 6 | 5.0 | False | 0.9692 | 0.8289  | 0.63 | 55s | 2019-16-06 23:07\n",
    "**20** | **2.2** | 6 | 5.0 | False | 0.9685 | **0.8334**  | 0.44 | 27s | 2019-16-06 23:17\n",
    "15 | 2.2 | 6 | 5.0 | False | 0.9685 | 0.8323  | 0.44 | 37s | 2019-16-06 23:33\n",
    "10 | 2.2 | 6 | 5.0 | False | 0.9689 | 0.8319  | 0.43 | 18s | 2019-16-06 23:41\n",
    "30 | 2.2 | 6 | 5.0 | False | 0.9681 | 0.8334  | 0.44 | 31s | 2019-16-06 23:49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 8 ms, total: 16 ms\n",
      "Wall time: 31.3 s\n"
     ]
    }
   ],
   "source": [
    "# Fit ALS on the training data\n",
    "als = ALS(maxIter=20, regParam=2.2, rank=6, coldStartStrategy=\"nan\", \\\n",
    "          userCol='user_id', itemCol='item_id', ratingCol='purchase', \\\n",
    "          nonnegative=False, implicitPrefs=True, alpha=5.0, seed=87)\n",
    "%time als_model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+\n",
      "|user_id|item_id|purchase|   prediction|\n",
      "+-------+-------+--------+-------------+\n",
      "| 746713|   8389|       0|          0.0|\n",
      "| 883098|   8389|       0|-0.0036029667|\n",
      "| 903491|   8389|       0| 0.0056212842|\n",
      "| 903826|   8389|       0|  0.037457652|\n",
      "| 916566|   8389|       0| 5.723099E-27|\n",
      "+-------+-------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "#Let see how the model perform on train set\n",
    "predict_train = als_model.transform(train)\n",
    "%time predict_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_train.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = predict_train.coalesce(4).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- prediction: float (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 329 µs\n"
     ]
    }
   ],
   "source": [
    "# We need DoubleType() in prediction column for the evaluator\n",
    "predict_train = predict_train.withColumn(\"prediction\", predict_train.prediction.cast(DoubleType()))\n",
    "%time predict_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 32.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, item_id: int, purchase: int, prediction: double]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time predict_train.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+--------------------+\n",
      "|summary|          user_id|           item_id|            purchase|          prediction|\n",
      "+-------+-----------------+------------------+--------------------+--------------------+\n",
      "|  count|          5032624|           5032624|             5032624|             5032624|\n",
      "|   mean|869680.9464782189| 66869.30485865823|0.002166662957534...| 0.00511500408708944|\n",
      "| stddev|60601.09821563049|35242.282055382544|0.046496977952915644|0.019791464685331724|\n",
      "|    min|             1654|               326|                   0|-0.21284088492393494|\n",
      "|    25%|           846231|             60351|                   0|-2.35493163927458...|\n",
      "|    50%|           885247|             79853|                   0|                 0.0|\n",
      "|    75%|           908726|             93602|                   0|0.003300537588074...|\n",
      "|    max|           941450|            104165|                   1| 0.45940831303596497|\n",
      "+-------+-----------------+------------------+--------------------+--------------------+\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 6.17 s\n"
     ]
    }
   ],
   "source": [
    "%time predict_train.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 10.8 s\n",
      "ROC AUC for train data: 0.9681270722320425\n"
     ]
    }
   ],
   "source": [
    "# check roc_auc on the train set\n",
    "%time rocauc_train = evaluator.evaluate(predict_train)\n",
    "print(f'ROC AUC for train data: {rocauc_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+\n",
      "|user_id|item_id|purchase|   prediction|\n",
      "+-------+-------+--------+-------------+\n",
      "| 740405|   8389|    null|-5.8040954E-4|\n",
      "| 838617|   8389|    null| 0.0054843277|\n",
      "| 916910|   8389|    null|-0.0045700995|\n",
      "| 814235|   8389|    null| 6.6968426E-4|\n",
      "| 849754|   8389|    null| -6.556209E-4|\n",
      "+-------+-------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "# predict test data\n",
    "predict_test = als_model.transform(test)\n",
    "%time predict_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = predict_test.coalesce(4).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+--------+--------------------+\n",
      "|summary|          user_id|          item_id|purchase|          prediction|\n",
      "+-------+-----------------+-----------------+--------+--------------------+\n",
      "|  count|          2156840|          2156840|       0|             2156840|\n",
      "|   mean|869652.3733920922|66896.00283609354|    null|0.005011935678396624|\n",
      "| stddev|60706.51616333814|35227.83130704636|    null| 0.01915306531477629|\n",
      "|    min|             1654|              326|    null|         -0.20226234|\n",
      "|    25%|           846231|            65667|    null|       -2.3715185E-4|\n",
      "|    50%|           885247|            79856|    null|                 0.0|\n",
      "|    75%|           908726|            93606|    null|        0.0032955278|\n",
      "|    max|           941450|           104165|    null|          0.44184247|\n",
      "+-------+-----------------+-----------------+--------+--------------------+\n",
      "\n",
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%time predict_test.summary().show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Отказался от обрезания\n",
    "predict_clean = predict_test \\\n",
    "            .withColumn(\"prediction\", when(col(\"prediction\") < 0.0, 0.0).otherwise(col(\"prediction\"))) \\\n",
    "            .withColumn(\"prediction\", when(col(\"prediction\") > 1.0, 1.0).otherwise(col(\"prediction\")))\n",
    "predict_clean.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Output as csv to hdfs & copy to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+\n",
      "|user_id|item_id|    purchase|\n",
      "+-------+-------+------------+\n",
      "|   1654|    336|         0.0|\n",
      "|   1654|    678|         0.0|\n",
      "|   1654|    691|         0.0|\n",
      "|   1654|    696| 7.360736E-4|\n",
      "|   1654|    763|0.0017997124|\n",
      "+-------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make output dataframe\n",
    "output = predict_test.select('user_id', 'item_id', col('prediction').alias('purchase')) \\\n",
    "                     .orderBy(['user_id', 'item_id'])\n",
    "output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 5.77 s\n"
     ]
    }
   ],
   "source": [
    "# write csv file with predictions\n",
    "%time output.coalesce(1).write.csv('/user/sergey.zaytsev/lab10', header=True, sep=',', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 sergey.zaytsev sergey.zaytsev          0 2019-06-16 23:48 /user/sergey.zaytsev/lab10/_SUCCESS\r\n",
      "-rw-r--r--   3 sergey.zaytsev sergey.zaytsev   50180152 2019-06-16 23:48 /user/sergey.zaytsev/lab10/part-00000-e431069f-4b6b-4ebd-b133-9e64c94d8d80-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/sergey.zaytsev/lab10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy output file to local directory\n",
    "!hadoop fs -copyToLocal /user/sergey.zaytsev/lab10/* ~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut down Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
